{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from prettytable import PrettyTable\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Read the jsonl file and convert it to a JSON list\n",
    "def jsonl_to_json_list(jsonl_file_path):\n",
    "    json_list = []\n",
    "    with open(jsonl_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line.strip())  # Parse each line as JSON\n",
    "            json_list.append(json_obj)\n",
    "    \n",
    "    return json_list\n",
    "\n",
    "# Save the JSON list to a file\n",
    "def save_as_json(json_list, output_file_path):\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(json_list, outfile, indent=4)\n",
    "\n",
    "def save_as_jsonl(json_list, output_file_path):\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        for json_obj in json_list:\n",
    "            json.dump(json_obj, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = [json.loads(line.strip()) for line in file]\n",
    "    return data\n",
    "\n",
    "def deduplicate_data(data):\n",
    "    seen = set()\n",
    "    deduplicated_data = []\n",
    "    for item in data:\n",
    "        idx = item['realidx']\n",
    "        if idx not in seen:\n",
    "            deduplicated_data.append(item)\n",
    "            seen.add(idx)\n",
    "    return deduplicated_data\n",
    "\n",
    "def calculate_accuracy(data):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(data)\n",
    "    for item in data:\n",
    "        if 'predicted_answer' not in item:\n",
    "            print(item['realidx'])\n",
    "        if item['answer_idx'] == item['predicted_answer']:\n",
    "            correct_predictions += 1\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "def calculate_cost_from_token_usage(data, model):\n",
    "    total_cost = 0\n",
    "    for item in data:\n",
    "        if 'cost' in item:\n",
    "            total_cost += item['cost']\n",
    "        elif model == 'gpt-4o-mini':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 0.15 / 1000000 + item['token_usage']['completion_tokens'] * 0.6 / 1000000\n",
    "        elif model == 'gpt-4o':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 2.5 / 1000000 + item['token_usage']['completion_tokens'] * 10 / 1000000\n",
    "        elif model == 'o3-mini' or model == 'o1-mini':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 1.1 / 1000000 + item['token_usage']['completion_tokens'] * 4.4 / 1000000\n",
    "        elif model == 'claude-3-5-sonnet':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 3.0 / 1000000 + item['token_usage']['completion_tokens'] * 15.0 / 1000000\n",
    "        elif model == 'claude-3-5-haiku':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 0.8 / 1000000 + item['token_usage']['completion_tokens'] * 4.0 / 1000000\n",
    "        elif model == 'QwQ-32B-Preview':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 1.2 / 1000000 + item['token_usage']['completion_tokens'] * 1.2 / 1000000\n",
    "        elif model == 'DeepSeek-R1':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 7 / 1000000 + item['token_usage']['completion_tokens'] * 7 / 1000000\n",
    "        elif model == 'DeepSeek-V3':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 1.25 / 1000000 + item['token_usage']['completion_tokens'] * 1.25 / 1000000\n",
    "        elif model == 'Llama-3.3-70B-Instruct-Turbo':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 0.88 / 1000000 + item['token_usage']['completion_tokens'] * 0.88 / 1000000\n",
    "    return total_cost / len(data)\n",
    "\n",
    "def calculate_time_from_data(data):\n",
    "    total_time = 0\n",
    "    for item in data:\n",
    "        total_time += item['time_elapsed']\n",
    "    return total_time / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Hard Questions (correct < 50% across models):\n",
      "+------------+--------------------------+\n",
      "|    Task    | Number of Hard Questions |\n",
      "+------------+--------------------------+\n",
      "|   medqa    |           100            |\n",
      "|  pubmedqa  |           100            |\n",
      "|  medmcqa   |           100            |\n",
      "| medbullets |            89            |\n",
      "|    mmlu    |            73            |\n",
      "|  mmlu-pro  |           100            |\n",
      "| afrimedqa  |            32            |\n",
      "+------------+--------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Create test_hard.json with questions that have low accuracy across models\n",
    "hard_questions = {}\n",
    "\n",
    "models = [\n",
    "    'gpt-4o-mini',\n",
    "    'gpt-4o', \n",
    "    'o1-mini',\n",
    "    'o3-mini',\n",
    "    'QwQ-32B-Preview',\n",
    "    'DeepSeek-R1',\n",
    "    'Llama-3.3-70B-Instruct-Turbo',\n",
    "    'DeepSeek-V3',\n",
    "    'claude-3-5-sonnet',\n",
    "    'claude-3-5-haiku'\n",
    "]\n",
    "methods = ['zero_shot']\n",
    "tasks = ['medqa', 'pubmedqa', 'medmcqa', 'medbullets', 'mmlu', 'mmlu-pro', 'afrimedqa']\n",
    "\n",
    "hard_bands = {}\n",
    "for task in tasks:\n",
    "    # Track correct answers per question across all models\n",
    "    question_correct_counts = {}\n",
    "    question_total_counts = {}\n",
    "    \n",
    "    # First pass - count correct answers for each question\n",
    "    for model in models:\n",
    "        for method in methods:\n",
    "            try:\n",
    "                file_path = f'../output/{task}/{model}-{task}-test-{method}.json'\n",
    "                data = load_json(file_path)\n",
    "                dedup_data = deduplicate_data(data)\n",
    "\n",
    "                for item in dedup_data:\n",
    "                    question = item['realidx']\n",
    "                    if question not in question_correct_counts:\n",
    "                        question_correct_counts[question] = 0\n",
    "                        question_total_counts[question] = 0\n",
    "\n",
    "                    # Increment total count\n",
    "                    question_total_counts[question] += 1\n",
    "                    \n",
    "                    # Check if answer was correct\n",
    "                    if item['answer_idx'] == item['predicted_answer']:\n",
    "                        question_correct_counts[question] += 1\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    hard_questions_task = []\n",
    "    file_path = f'../data/{task}/test.jsonl'\n",
    "    data = load_jsonl(file_path)\n",
    "    for problem in data:\n",
    "        total_count = question_total_counts[problem['realidx']]\n",
    "        correct_count = question_correct_counts[problem['realidx']]\n",
    "        if correct_count / total_count < 0.5:  # Less than 5/10 correct\n",
    "            hard_questions_task.append(problem)\n",
    "    \n",
    "    # Truncate to 100 questions if more than 100\n",
    "    if len(hard_questions_task) > 100:\n",
    "        hard_questions_task = hard_questions_task[:100]\n",
    "\n",
    "    hard_correct_ratios = []\n",
    "    for q in hard_questions_task:\n",
    "        ratio = question_correct_counts[q['realidx']] / question_total_counts[q['realidx']]\n",
    "        hard_correct_ratios.append(ratio)\n",
    "    \n",
    "    # Count hard questions in each correctness band\n",
    "    hard_bands[task] = {\n",
    "        \"0/10\": len([r for r in hard_correct_ratios if r < 0.1]),\n",
    "        \"1/10\": len([r for r in hard_correct_ratios if 0.1 <= r < 0.2]), \n",
    "        \"2/10\": len([r for r in hard_correct_ratios if 0.2 <= r < 0.3]),\n",
    "        \"3/10\": len([r for r in hard_correct_ratios if 0.3 <= r < 0.4]),\n",
    "        \"4/10\": len([r for r in hard_correct_ratios if 0.4 <= r < 0.5]),\n",
    "        \"5/10\": len([r for r in hard_correct_ratios if 0.5 <= r < 0.6]),\n",
    "        \"6/10\": len([r for r in hard_correct_ratios if 0.6 <= r < 0.7]),\n",
    "        \"7/10\": len([r for r in hard_correct_ratios if 0.7 <= r < 0.8]),\n",
    "        \"8/10\": len([r for r in hard_correct_ratios if 0.8 <= r < 0.9]),\n",
    "        \"9/10\": len([r for r in hard_correct_ratios if 0.9 <= r < 1.0]),\n",
    "        \"10/10\": len([r for r in hard_correct_ratios if r >= 1.0])\n",
    "    }\n",
    "\n",
    "    if hard_questions_task:\n",
    "        if task not in hard_questions:\n",
    "            hard_questions[task] = {}\n",
    "        hard_questions[task] = hard_questions_task\n",
    "\n",
    "    # save_as_jsonl(hard_questions[task], f'../../data/{task}/test_hard.jsonl')\n",
    "\n",
    "# Print summary table\n",
    "summary_table = PrettyTable()\n",
    "summary_table.field_names = [\"Task\", \"Number of Hard Questions\"]\n",
    "\n",
    "for task in hard_questions:\n",
    "    num_hard = len(hard_questions[task])\n",
    "    summary_table.add_row([task, num_hard])\n",
    "\n",
    "print(\"\\nSummary of Hard Questions (correct < 50% across models):\")\n",
    "print(summary_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 'medqa' with 10 model file(s).\n",
      "Processing dataset 'medmcqa' with 10 model file(s).\n",
      "Processing dataset 'pubmedqa' with 10 model file(s).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Set different font families for different elements\n",
    "plt.rcParams['font.family'] = 'Courier New'  # Base font\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "\n",
    "# Custom font settings\n",
    "title_font = {'family': 'Courier New', 'fontsize': 16, 'fontweight': 'bold'}\n",
    "legend_font = {'family': 'Courier New', 'fontsize': 8, 'fontweight': 'bold'}\n",
    "number_font = {'family': 'Courier New', 'fontsize': 10}\n",
    "axis_font = {'family': 'Courier New', 'fontsize': 12, 'fontweight': 'bold'}\n",
    "\n",
    "# Assume that both 'datasets' and 'model_names' are defined previously.\n",
    "datasets = ['medqa', 'medmcqa', 'pubmedqa', 'medbullets', 'mmlu-pro', 'mmlu', 'afrimedqa']\n",
    "dataset_map = {\n",
    "    'medqa': 'MedQA',\n",
    "    'pubmedqa': 'PubMedQA',\n",
    "    'medmcqa': 'MedMCQA',\n",
    "    'medbullets': 'MedBullets',\n",
    "    'mmlu': 'MMLU',\n",
    "    'mmlu-pro': 'MMLU-Pro',\n",
    "    'afrimedqa': 'AfrimedQA',\n",
    "    'mmlu-pro': 'MMLU-Pro',\n",
    "    'afrimedqa': 'AfrimedQA'\n",
    "}\n",
    "# model_names is assumed to be defined earlier (e.g., model_names = ['claude-3-5-haiku', 'claude-3-5-sonnet', ...])\n",
    "\n",
    "# Determine subplot grid dimensions (using 3 columns for a balanced layout)\n",
    "n_datasets = len(datasets)\n",
    "ncols = 3\n",
    "nrows = math.ceil(n_datasets / ncols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5 * ncols, 4 * nrows), dpi=300)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create empty lists to store legend handles and labels\n",
    "legend_handles = []\n",
    "legend_labels = []\n",
    "\n",
    "for ax, dataset in zip(axes, datasets):\n",
    "    print(f\"Processing dataset '{dataset}' with {len(models)} model file(s).\")\n",
    "    \n",
    "    # Aggregate the count of models that answered each question correctly.\n",
    "    # Use the question's unique identifier (\"realidx\") if available; otherwise, fallback to \"question\" text.\n",
    "    question_correct = {}  # key: question identifier; value: count of correct responses across models\n",
    "    \n",
    "    for model in models:\n",
    "        # Convert model name to a file-friendly format.\n",
    "        model_filename = model.replace('/', '_')\n",
    "        # Construct the expected JSON filename.\n",
    "        file_path = os.path.join('..', 'output', dataset, f\"{model_filename}-{dataset}-test-zero_shot.json\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"JSON file not found for model '{model}' in dataset '{dataset}' with expected filename: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            try:\n",
    "                predictions = json.load(f)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from {file_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        for entry in predictions:\n",
    "            # Use \"realidx\" as unique identifier if provided; otherwise, fallback to the \"question\" text.\n",
    "            qid = entry.get(\"realidx\") or entry.get(\"question\", \"unknown_question\")\n",
    "            # Determine whether the model's answer is correct.\n",
    "            predicted = str(entry.get(\"predicted_answer\", \"\")).strip()\n",
    "            correct_ans = str(entry.get(\"answer_idx\", \"\")).strip()\n",
    "            # Initialize count for this question if not already present.\n",
    "            if qid not in question_correct:\n",
    "                question_correct[qid] = 0\n",
    "            if predicted == correct_ans:\n",
    "                question_correct[qid] += 1\n",
    "\n",
    "    # Assume the total number of models equals the length of model_names.\n",
    "    total_models = len(models)\n",
    "    \n",
    "    # Compute the distribution: count how many questions received k correct answers (k = 0, 1, ..., total_models).\n",
    "    distribution = Counter(question_correct.values())\n",
    "    \n",
    "    # Prepare data for plotting: ensure counts for all possible correct responses are present.\n",
    "    x_values = list(range(total_models + 1))\n",
    "    y_counts = [distribution.get(k, 0) for k in x_values]\n",
    "    y_counts_selected = [hard_bands[dataset][f\"{k}/{total_models}\"] for k in x_values]\n",
    "    y_counts_unselected = [y_counts[i] - y_counts_selected[i] for i in range(len(x_values))]\n",
    "    x_labels = [f\"{k}/{total_models}\" for k in x_values]\n",
    "\n",
    "    # Create a stacked bar plot in the corresponding subplot axis, stacking the 'selected' and 'unselected' counts.\n",
    "    colors_selected = ['crimson' if k/total_models < 0.5 else 'skyblue' for k in x_values]\n",
    "    # Use a lighter shade for the unselected portion.\n",
    "    colors_unselected = ['lightcoral' if k/total_models < 0.5 else 'lightsteelblue' for k in x_values]\n",
    "    selected_bars = ax.bar(x_values, y_counts_selected, color=colors_selected, edgecolor='black', label='Selected')\n",
    "    unselected_bars = ax.bar(x_values, y_counts_unselected, bottom=y_counts_selected, color=colors_unselected, edgecolor='black', label='Unselected')\n",
    "    \n",
    "    # Add vertical dashed line at x = total_models/2 to separate hard and easy regions\n",
    "    ax.axvline(x=(total_models - 1)/2, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel(\"Models Correct\", fontdict=axis_font)\n",
    "    ax.set_ylabel(\"Number of Questions\", fontdict=axis_font)\n",
    "    ax.set_title(f\"{dataset_map[dataset]} (Total Qs: {sum(y_counts)})\", fontdict=title_font)\n",
    "    ax.set_xticks(x_values)\n",
    "    ax.set_xticklabels(x_labels, rotation=45, fontdict=number_font)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Annotate each bar with its count value for added clarity.\n",
    "    for ubar, sbar in zip(unselected_bars, selected_bars):\n",
    "        uheight = ubar.get_height()\n",
    "        sheight = sbar.get_height()\n",
    "        if uheight > 0:\n",
    "            ax.text(ubar.get_x() + ubar.get_width() / 2, max(uheight, 25 if sheight > 0 else 0) + sheight, f'{int(uheight) + int(sheight)}', \n",
    "                   ha='center', va='bottom', fontdict=number_font)\n",
    "        if sheight > 0:\n",
    "            ax.text(sbar.get_x() + sbar.get_width() / 2, sheight, f'{int(sheight)}', \n",
    "                   ha='center', va='bottom', fontdict=number_font)\n",
    "\n",
    "# Hide any unused subplots.\n",
    "for ax in axes[n_datasets:]:\n",
    "    ax.axis('off')\n",
    "\n",
    "# Add a single legend for all subplots\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='crimson', lw=4, label='Selected (Hard)'),\n",
    "    Line2D([0], [0], color='lightcoral', lw=4, label='Unselected (Hard)'),\n",
    "    Line2D([0], [0], color='lightsteelblue', lw=4, label='Easy'),\n",
    "    Line2D([0], [0], color='black', linestyle='--', lw=4, label='Hard/Easy Boundary')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='center', bbox_to_anchor=(0.5, 0.93), ncol=4, prop={'size': 14, 'weight': 'bold'})\n",
    "\n",
    "fig.suptitle(\"Distribution of Correct Answers Across Datasets\", weight='bold', fontsize=20)\n",
    "fig.tight_layout(rect=[0, 0, 0.95, 0.95])  # Adjust layout to make room for legend\n",
    "plt.show()\n",
    "plt.savefig('../assets/hardset.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
