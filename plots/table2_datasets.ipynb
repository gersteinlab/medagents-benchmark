{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from prettytable import PrettyTable\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "# Read the jsonl file and convert it to a JSON list\n",
    "def jsonl_to_json_list(jsonl_file_path):\n",
    "    json_list = []\n",
    "    with open(jsonl_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line.strip())  # Parse each line as JSON\n",
    "            json_list.append(json_obj)\n",
    "    \n",
    "    return json_list\n",
    "\n",
    "# Save the JSON list to a file\n",
    "def save_as_json(json_list, output_file_path):\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(json_list, outfile, indent=4)\n",
    "\n",
    "def save_as_jsonl(json_list, output_file_path):\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        for json_obj in json_list:\n",
    "            json.dump(json_obj, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = [json.loads(line.strip()) for line in file]\n",
    "    return data\n",
    "\n",
    "def deduplicate_data(data):\n",
    "    seen = set()\n",
    "    deduplicated_data = []\n",
    "    for item in data:\n",
    "        idx = item['realidx']\n",
    "        if idx not in seen:\n",
    "            deduplicated_data.append(item)\n",
    "            seen.add(idx)\n",
    "    return deduplicated_data\n",
    "\n",
    "def calculate_accuracy(data):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(data)\n",
    "    for item in data:\n",
    "        if 'predicted_answer' not in item:\n",
    "            print(item['realidx'])\n",
    "        if item['answer_idx'] == item['predicted_answer']:\n",
    "            correct_predictions += 1\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "def calculate_cost_from_token_usage(data, model):\n",
    "    total_cost = 0\n",
    "    for item in data:\n",
    "        if 'cost' in item:\n",
    "            total_cost += item['cost']\n",
    "        elif model == 'gpt-4o-mini':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 0.15 / 1000000 + item['token_usage']['completion_tokens'] * 0.6 / 1000000\n",
    "        elif model == 'gpt-4o':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 2.5 / 1000000 + item['token_usage']['completion_tokens'] * 10 / 1000000\n",
    "        elif model == 'o3-mini' or model == 'o1-mini':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 1.1 / 1000000 + item['token_usage']['completion_tokens'] * 4.4 / 1000000\n",
    "        elif model == 'claude-3-5-sonnet':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 3.0 / 1000000 + item['token_usage']['completion_tokens'] * 15.0 / 1000000\n",
    "        elif model == 'claude-3-5-haiku':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 0.8 / 1000000 + item['token_usage']['completion_tokens'] * 4.0 / 1000000\n",
    "        elif model == 'QwQ-32B-Preview':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 1.2 / 1000000 + item['token_usage']['completion_tokens'] * 1.2 / 1000000\n",
    "        elif model == 'DeepSeek-R1':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 7 / 1000000 + item['token_usage']['completion_tokens'] * 7 / 1000000\n",
    "        elif model == 'DeepSeek-V3':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 1.25 / 1000000 + item['token_usage']['completion_tokens'] * 1.25 / 1000000\n",
    "        elif model == 'Llama-3.3-70B-Instruct-Turbo':\n",
    "            total_cost += item['token_usage']['prompt_tokens'] * 0.88 / 1000000 + item['token_usage']['completion_tokens'] * 0.88 / 1000000\n",
    "    return total_cost / len(data)\n",
    "\n",
    "def calculate_time_from_data(data):\n",
    "    total_time = 0\n",
    "    for item in data:\n",
    "        total_time += item['time_elapsed']\n",
    "    return total_time / len(data)\n",
    "\n",
    "def calculate_token_length(text):\n",
    "    encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "    return len(encoding.encode(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table*}[h]\n",
      "\\begin{tabular}{lrrrp{5cm}}\n",
      "\\hline\n",
      "Benchmark & Size & Avg Lens & Options & Description \\\\\n",
      "\\hline\n",
      "\\rowcolor{blue!10}MedQA \\cite{jin2021medqa} & 1273 & 167.1 & 4 & Multiple choice questions from medical licensing exams \\\\\n",
      "\\rowcolor{blue!10}PubMedQA \\cite{jin2019pubmedqa} & 500 & 316.1 & 3 & Questions based on PubMed abstracts \\\\\n",
      "\\rowcolor{blue!10}MedMCQA \\cite{pal2022medmcqa} & 2816 & 18.7 & 4 & Questions from AIIMS \\& NEET PG entrance exams \\\\\n",
      "\\rowcolor{green!10}MedBullets \\cite{chen2024medbullet} & 308 & 213.1 & 5 & Questions from Medbullets online medical study platform \\\\\n",
      "\\rowcolor{green!10}Afrimed-QA \\cite{olatunji2024afrimed} & 174 & 30.0 & 5 & Diverse medical questions from African healthcare contexts \\\\\n",
      "\\rowcolor{orange!10}MMLU \\cite{hendrycks2020mmlu} & 1089 & 55.9 & 4 & Multitask questions covering medical, biology, and other academic domains \\\\\n",
      "\\rowcolor{orange!10}MMLU-Pro \\cite{wang2024mmlu} & 818 & 57.4 & 3-10 & Multitask questions covering medical, biology, and other academic domains \\\\\n",
      "\\hline\n",
      "\\Ours{} & 594 & 134.8 & 3-10 & Hard subset across all datasets \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{\\textbf{Overview of Medical Question-Answering Datasets.} Survey of knowledge-based QA datasets curated from medical literature, professional journals, and educational resources. \\colorbox{blue!10}{Traditional benchmarks}, \\colorbox{green!10}{recently emerging benchmarks}, and \\colorbox{orange!10}{general purpose benchmarks} are shown with corresponding background colors.}\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tasks = ['medqa', 'pubmedqa', 'medmcqa', 'medbullets', 'afrimedqa', 'mmlu', 'mmlu-pro']\n",
    "tasks_name_mapping = {\n",
    "    'medqa': 'MedQA \\\\cite{jin2021medqa}',\n",
    "    'pubmedqa': 'PubMedQA \\\\cite{jin2019pubmedqa}',\n",
    "    'medmcqa': 'MedMCQA \\\\cite{pal2022medmcqa}',\n",
    "    'medbullets': 'MedBullets \\\\cite{chen2024medbullet}',\n",
    "    'mmlu': 'MMLU \\\\cite{hendrycks2020mmlu}',\n",
    "    'mmlu-pro': 'MMLU-Pro \\\\cite{wang2024mmlu}',\n",
    "    'afrimedqa': 'Afrimed-QA \\\\cite{olatunji2024afrimed}'\n",
    "}\n",
    "tasks_description = {\n",
    "    'medqa': 'Multiple choice questions from medical licensing exams',\n",
    "    'pubmedqa': 'Questions based on PubMed abstracts',\n",
    "    'medmcqa': 'Questions from AIIMS \\\\& NEET PG entrance exams',\n",
    "    'medbullets': 'Questions from Medbullets online medical study platform',\n",
    "    'afrimedqa': 'Diverse medical questions from African healthcare contexts',\n",
    "    'mmlu': 'Multitask questions covering medical, biology, and other academic domains',\n",
    "    'mmlu-pro': 'Multitask questions covering medical, biology, and other academic domains',\n",
    "}\n",
    "\n",
    "latex_table = r\"\"\"\n",
    "\\begin{table*}[h]\n",
    "\\begin{tabular}{lrrrp{5cm}}\n",
    "\\hline\n",
    "Benchmark & Size & Avg Lens & Options & Description \\\\\n",
    "\\hline\n",
    "\"\"\"\n",
    "\n",
    "test_hard = []\n",
    "for task in tasks:\n",
    "    test = load_jsonl(f'../data/{task}/test.jsonl')\n",
    "    test_hard.extend(load_jsonl(f'../data/{task}/test_hard.jsonl'))\n",
    "    len_test = len(test)\n",
    "    avg_question_length = sum(calculate_token_length(item['question']) for item in test) / len_test\n",
    "    num_of_options = [len(item['options']) for item in test]\n",
    "    min_options = min(num_of_options)\n",
    "    max_options = max(num_of_options)\n",
    "    options_range = f\"{min_options}-{max_options}\" if min_options != max_options else min_options\n",
    "    \n",
    "    # Add color based on benchmark type\n",
    "    if task in ['medqa', 'pubmedqa', 'medmcqa']:\n",
    "        color = '\\\\rowcolor{blue!10}'  # Traditional benchmarks\n",
    "    elif task in ['medbullets', 'afrimedqa']:\n",
    "        color = '\\\\rowcolor{green!10}'  # Recently emerging benchmarks\n",
    "    else:\n",
    "        color = '\\\\rowcolor{orange!10}'  # General purpose benchmarks\n",
    "        \n",
    "    latex_table += f\"{color}{tasks_name_mapping[task]} & {len_test} & {avg_question_length:.1f} & {options_range} & {tasks_description[task]} \\\\\\\\\\n\"\n",
    "\n",
    "# Calculate stats for test_hard subset\n",
    "len_test_hard = len(test_hard)\n",
    "avg_question_length_hard = sum(calculate_token_length(item['question']) for item in test_hard) / len_test_hard\n",
    "num_of_options_hard = [len(item['options']) for item in test_hard]\n",
    "min_options_hard = min(num_of_options_hard)\n",
    "max_options_hard = max(num_of_options_hard)\n",
    "options_range_hard = f\"{min_options_hard}-{max_options_hard}\" if min_options_hard != max_options_hard else min_options_hard\n",
    "\n",
    "latex_table += r\"\\hline\" + \"\\n\"\n",
    "latex_table += f\"\\\\Ours{{}} & {len_test_hard} & {avg_question_length_hard:.1f} & {options_range_hard} & Hard subset across all datasets \\\\\\\\\\n\"\n",
    "\n",
    "latex_table += r\"\"\"\\hline\n",
    "\\end{tabular}\n",
    "\\caption{\\textbf{Overview of Medical Question-Answering Datasets.} Survey of knowledge-based QA datasets curated from medical literature, professional journals, and educational resources. \\colorbox{blue!10}{Traditional benchmarks}, \\colorbox{green!10}{recently emerging benchmarks}, and \\colorbox{orange!10}{general purpose benchmarks} are shown with corresponding background colors.}\n",
    "\\end{table*}\n",
    "\"\"\"\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{table*}[h]\n",
      "\\begin{tabular}{lp{8cm}}\n",
      "\\hline\n",
      "Method & Description \\\\\n",
      "\\hline\n",
      "\\rowcolor{blue!10}Chain-of-Thought \\cite{wei2022chain} & Elicits reasoning in large language models \\\\\n",
      "\\rowcolor{blue!10}Self-Consistency \\cite{wang2022self} & Improves chain of thought reasoning in language models \\\\\n",
      "\\rowcolor{blue!10}MedPrompt \\cite{chen2024medprompt} & Multi-round prompting with ensemble voting for medical question answering \\\\\n",
      "\\rowcolor{blue!10}Multi-Persona \\cite{wang2023multipersona} & Task-solving agent through multi-persona self-collaboration \\\\\n",
      "\\rowcolor{blue!10}Self-Refine \\cite{madaan2024selfrefine} & Iterative refinement with self-feedback \\\\\n",
      "\\rowcolor{green!10}MedAgents \\cite{tang2023medagents} & Collaborative multi-agent framework for zero-shot medical decision making \\\\\n",
      "\\rowcolor{green!10}MDAgents \\cite{kim2024mdagents} & Dynamic multi-agent collaboration framework for medical reasoning \\\\\n",
      "\\rowcolor{orange!10}AFlow \\cite{zhang2024aflow} & Automating agentic workflow generation \\\\\n",
      "\\rowcolor{orange!10}SPO \\cite{xiang2025spo} & Self-supervised prompt optimization \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\\caption{\\textbf{Overview of Methods.} Survey of methods used for medical reasoning and question answering. \\colorbox{blue!10}{General-purpose methods}, \\colorbox{green!10}{domain-specific methods}, and \\colorbox{orange!10}{search-based methods} are shown with corresponding background colors.}\n",
      "\\end{table*}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "methods = [\n",
    "    # General-purpose methods\n",
    "    ('Chain-of-Thought', 'wei2022chain', 'Elicits reasoning in large language models'),\n",
    "    ('Self-Consistency', 'wang2022self', 'Improves chain of thought reasoning in language models'),\n",
    "    ('MedPrompt', 'chen2024medprompt', 'Multi-round prompting with ensemble voting for medical question answering'),\n",
    "    ('Multi-Persona', 'wang2023multipersona', 'Task-solving agent through multi-persona self-collaboration'),\n",
    "    ('Self-Refine', 'madaan2024selfrefine', 'Iterative refinement with self-feedback'),\n",
    "    \n",
    "    # Domain-specific methods\n",
    "    ('MedAgents', 'tang2023medagents', 'Collaborative multi-agent framework for zero-shot medical decision making'),\n",
    "    ('MDAgents', 'kim2024mdagents', 'Dynamic multi-agent collaboration framework for medical reasoning'),\n",
    "    \n",
    "    # Search-based methods\n",
    "    ('AFlow', 'zhang2024aflow', 'Automating agentic workflow generation'),\n",
    "    ('SPO', 'xiang2025spo', 'Self-supervised prompt optimization')\n",
    "]\n",
    "\n",
    "latex_table = r\"\"\"\n",
    "\\begin{table*}[h]\n",
    "\\begin{tabular}{lp{8cm}}\n",
    "\\hline\n",
    "Method & Description \\\\\n",
    "\\hline\n",
    "\"\"\"\n",
    "\n",
    "for method, citation, desc in methods:\n",
    "    # Add color based on method type\n",
    "    if method in ['Chain-of-Thought', 'Self-Consistency', 'MedPrompt', 'Multi-Persona', 'Self-Refine']:\n",
    "        color = '\\\\rowcolor{blue!10}'  # General-purpose methods\n",
    "    elif method in ['MedAgents', 'MDAgents']:\n",
    "        color = '\\\\rowcolor{green!10}'  # Domain-specific methods\n",
    "    else:\n",
    "        color = '\\\\rowcolor{orange!10}'  # Search-based methods\n",
    "        \n",
    "    latex_table += f\"{color}{method} \\\\cite{{{citation}}} & {desc} \\\\\\\\\\n\"\n",
    "\n",
    "latex_table += r\"\"\"\\hline\n",
    "\\end{tabular}\n",
    "\\caption{\\textbf{Overview of Methods.} Survey of methods used for medical reasoning and question answering. \\colorbox{blue!10}{General-purpose methods}, \\colorbox{green!10}{domain-specific methods}, and \\colorbox{orange!10}{search-based methods} are shown with corresponding background colors.}\n",
    "\\end{table*}\n",
    "\"\"\"\n",
    "print(latex_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
